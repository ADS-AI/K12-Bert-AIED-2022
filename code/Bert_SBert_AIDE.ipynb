{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5f6f766",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "import torch\n",
    "import logging\n",
    "import time\n",
    "import datetime\n",
    "import json\n",
    "logging.basicConfig(level=logging.ERROR)\n",
    "# If there's a GPU available...\n",
    "if torch.cuda.is_available():    \n",
    "\n",
    "    # Tell PyTorch to use the GPU.    \n",
    "    device = torch.device(\"cuda\")\n",
    "\n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "\n",
    "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
    "\n",
    "# # If not...\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "# device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46282184",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install tensorflow==1.13.1\n",
    "# ! pip install tensorflow-hub==0.7.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a969f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install git+https://github.com/geoopt/geoopt.git\n",
    "# ! pip install git+https://github.com/ferrine/hyrnn.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00e59f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# ! pip install seaborn\n",
    "# ! pip install transformers\n",
    "# ! pip install sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "648fc0ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from transformers import ElectraConfig, ElectraTokenizerFast, ElectraForMaskedLM, ElectraForPreTraining\n",
    "import random\n",
    "import numpy as np\n",
    "import json\n",
    "from sklearn.metrics import f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "# % matplotlib inline\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57d9985d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_sentence(question):\n",
    "  # print(question)\n",
    "  question = re.sub('<[^>]*>', ' ',question)\n",
    "  question = re.sub(' +', ' ', question)\n",
    "  question = re.sub('\\xa0','',question)\n",
    "  question = question.rstrip()\n",
    "  question = re.sub('nan','',question)\n",
    "  question = re.sub(u'\\u2004','',question)\n",
    "  question = re.sub(u'\\u2009','',question)\n",
    "\n",
    "  # question = question.decode(\"utf-8\")\n",
    "  # question = question.replace(u'\\u200\\d*','').encode(\"utf-8\")\n",
    "  question = re.sub('&nbsp','',question)\n",
    "  question = re.sub('&ndash','',question)\n",
    "  question = re.sub('\\r','',question)\n",
    "  question = re.sub('\\t','',question)\n",
    "  question = re.sub('\\n',' ',question)\n",
    "\n",
    "  question = re.sub('MathType@.*','',question)\n",
    "  question = re.sub('&thinsp','',question)\n",
    "  question = re.sub('&times','',question)\n",
    "  question = re.sub('\\u200b','',question)\n",
    "  question = re.sub('&rarr;;;','',question)\n",
    "\n",
    "  return question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "473d1e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # !pip install inflection bokeh\n",
    "# !pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c1dbd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from bokeh.io import output_file, output_notebook, show\n",
    "from bokeh.plotting import figure\n",
    "from bokeh.transform import linear_cmap\n",
    "from bokeh.util.hex import hexbin\n",
    "from bokeh.models import HoverTool\n",
    "from bokeh import colors\n",
    "import inflection\n",
    "from nltk.stem import PorterStemmer \n",
    "ps = PorterStemmer()\n",
    "from gzip import open as gopen\n",
    "from pandas.core.common import flatten\n",
    "from torch.utils.data import TensorDataset, random_split\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "import gensim.models.poincare as poincare\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "def get_cleaned_taxonomy(taxonomy, D):\n",
    "  splitter = '>>'\n",
    "  if D == 'ARC':\n",
    "    splitter = '_'\n",
    "  cleaned_taxonomy = []\n",
    "  for value in taxonomy:\n",
    "      value = ' '.join(value.lower().split(splitter))\n",
    "      # taxonomy_words = [inflection.singularize(val)  for token in value for val in token.split(\" \") if val.isalpha()]\n",
    "      cleaned_taxonomy.append( value )\n",
    "  return cleaned_taxonomy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcf7ff14",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['TFHUB_CACHE_DIR'] = '/home/vasu/tmp'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43cd1d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_hub as hub\n",
    "import tensorflow as tf\n",
    "# # import tensorflow.compat.v1 as tf\n",
    "# # tf.disable_v2_behavior()\n",
    "# class UseSentenceEmbedding():\n",
    "#     def __init__(self):\n",
    "#         # g = tf.Graph()\n",
    "#         print(\"HERE\")\n",
    "#         with tf.device('/CPU:0'):\n",
    "#         # We will be feeding 1D tensors of text into the graph.\n",
    "#             self.text_input = tf.placeholder(dtype=tf.string, shape=[None])\n",
    "\n",
    "#             #kindly replace the location in hub.module with the url commented out below\n",
    "#             print(\"HERE\")\n",
    "#             # \"https://tfhub.dev/google/universal-sentence-encoder-large/3\"\n",
    "#             embed = hub.Module(\"https://tfhub.dev/google/universal-sentence-encoder-large/3\")\n",
    "#             self.embedded_text = embed(self.text_input)\n",
    "#             init_op = tf.group([tf.global_variables_initializer(), tf.tables_initializer()])\n",
    "#         # g.finalize()\n",
    "#         print(\"HERE\")\n",
    "#         self.session = tf.Session(config=tf.ConfigProto( allow_soft_placement=True))\n",
    "#         self.session.run(init_op)\n",
    "#         print(\"init _____\")\n",
    "\n",
    "\n",
    "\n",
    "#     def get_tokenized_sents_embeddings_USE(self, sents,expand=False):\n",
    "\n",
    "\n",
    "#         vectors_USE =  self.session.run(self.embedded_text, feed_dict={self.text_input: sents})\n",
    "\n",
    "#         return vectors_USE\n",
    "\n",
    "# use_embedding = UseSentenceEmbedding()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73e55ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "sbert_path = './sbert_main_model/'\n",
    "# sbert = SentenceTransformer(sbert_path)\n",
    "sbert = SentenceTransformer('bert-large-nli-stsb-mean-tokens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e450d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "! which python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5120ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from transformers import ElectraModel, AdamW, ElectraConfig, BertTokenizerFast, BertTokenizer\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import BertModel, AdamW, BertConfig\n",
    "from transformers import get_linear_schedule_with_warmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "074a94f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import models\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from matplotlib import pyplot as plt\n",
    "from torch.nn.modules.loss import HingeEmbeddingLoss\n",
    "from random import randint\n",
    "\n",
    "from tqdm import tqdm\n",
    "import geoopt\n",
    "import time\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from matplotlib import pyplot as plt\n",
    "from torch.nn.modules.loss import HingeEmbeddingLoss\n",
    "from random import randint\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import time\n",
    "import argparse\n",
    "cos = nn.CosineSimilarity(dim=0, eps=1e-6)\n",
    "# Neural Classifierwork\n",
    "class MulticlassClassifier(nn.Module):\n",
    "    def __init__(self,model_path, model, state_dict_path=None):\n",
    "        super(MulticlassClassifier,self).__init__()\n",
    "#         print(bert_model_path)\n",
    "        print(model)\n",
    "        self.model = model.from_pretrained(model_path)#,output_hidden_states=True,output_attentions=False)\n",
    "#         print(self.model.state_dict())\n",
    "#         print(self.model)\n",
    "#         if state_dict_path is not None:\n",
    "#             model.load_state_dict(state_dict=torch.load(state_dict_path))\n",
    "#             print(model.state_dict())\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.fc1 = nn.Linear(768, 384)\n",
    "        self.fc2 = nn.Linear(384, 512)\n",
    "\n",
    "    def forward(self,tokens,masks):\n",
    "        print(self.model(tokens, attention_mask=masks))\n",
    "        _, pooled_output,hidden_states = self.model(tokens, attention_mask=masks)\n",
    "        representation = torch.mean(hidden_states[-2],dim=1)\n",
    "        x = self.fc1(representation)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "    \n",
    "class CustomClassifier(nn.Module):\n",
    "    def __init__(self,model_path, model, state_dict_path=None):\n",
    "        super(CustomClassifier,self).__init__()\n",
    "#         print(bert_model_path)\n",
    "#         print(model)\n",
    "        self.electra = model.from_pretrained(model_path,output_hidden_states=True,output_attentions=False)\n",
    "        if state_dict_path:\n",
    "            print(\"embeddings loaded\")\n",
    "            state_dict = load_part_state_dict(state_dict_path, 'discriminator')\n",
    "            self.electra.load_state_dict(state_dict)\n",
    "            self.electra.cuda()\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.fc1 = nn.Linear(768, 384)\n",
    "        self.fc2 = nn.Linear(384, 1024)\n",
    "\n",
    "    def forward(self,tokens,masks):\n",
    "#         print(self.electra(tokens, attention_mask=masks))\n",
    "        X = self.electra(tokens, attention_mask=masks)\n",
    "#         print(x)\n",
    "        hidden_states = X.hidden_states\n",
    "#         print(x.shape)\n",
    "        representation = torch.mean(hidden_states[-2],dim=1)\n",
    "#         return self.fc2(self.fc1(self.dropout(x[:,0,:]))).squeeze(-1).float()\n",
    "#         dropped = self.dropout(representation)\n",
    "        x = self.fc1(representation)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "class MyHingeLoss(torch.nn.Module):\n",
    "    def __init__(self, margin):\n",
    "        super(MyHingeLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "    # def forward_val(self, output, target):\n",
    "    #     cos = nn.CosineSimilarity(dim=0, eps=1e-6)\n",
    "    #     loss = 0\n",
    "    #     num_compare = 4\n",
    "    #     count = 0\n",
    "    #     for i in range(len(output)):\n",
    "    #         v_image = output[i]\n",
    "    #         t_label = target[i]\n",
    "    #         for j in range(num_compare):\n",
    "    #             if j != i:\n",
    "    #                 count += 1\n",
    "    #                 t_j = target[j]\n",
    "    #                 loss += torch.relu( self.margin - cos(t_label, v_image) + cos(t_j, v_image) )\n",
    "    #     return loss / count\n",
    "\n",
    "    def forward(self, output, target):\n",
    "        loss=0\n",
    "        for i in range(len(output)):\n",
    "            v_image = F.normalize(output[i],p=2,dim=0)\n",
    "            t_label = F.normalize(target[i],p=2,dim=0)\n",
    "            j = randint(0, len(output)-1)\n",
    "            while j == i:\n",
    "                j = randint(0, len(output)-1)\n",
    "            t_j = F.normalize(target[j],p=2,dim=0)\n",
    "            loss+= torch.relu( self.margin - cos(t_label, v_image) + cos(t_j, v_image) )\n",
    "        return loss / len(output)\n",
    "\n",
    "def load_part_state_dict(file, prefix, device=None, strict=True):\n",
    "  \"assume `model` is part of (child attribute at any level) of model whose states save in `file`.\"\n",
    "#   distrib_barrier()\n",
    "  if prefix[-1] != '.': prefix += '.'\n",
    "  if isinstance(device, int): device = torch.device('cuda', device)\n",
    "  elif device is None: device = 'cpu'\n",
    "  state = torch.load(file, map_location=device)\n",
    "  hasopt = set(state)=={'model', 'opt'}\n",
    "  model_state = state['model'] if hasopt else state\n",
    "  model_state = {k[len(prefix):] : v for k,v in model_state.items() if k.startswith(prefix)}\n",
    "  return model_state\n",
    "\n",
    "\n",
    "class EarlyStopping:\n",
    "    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n",
    "    def __init__(self, patience=7, verbose=False, delta=0, path='checkpoint.pt', trace_func=print):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            patience (int): How long to wait after last time validation loss improved.\n",
    "                            Default: 7\n",
    "            verbose (bool): If True, prints a message for each validation loss improvement. \n",
    "                            Default: False\n",
    "            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n",
    "                            Default: 0\n",
    "            path (str): Path for the checkpoint to be saved to.\n",
    "                            Default: 'checkpoint.pt'\n",
    "            trace_func (function): trace print function.\n",
    "                            Default: print            \n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = np.Inf\n",
    "        self.delta = delta\n",
    "        self.path = path\n",
    "        self.trace_func = trace_func\n",
    "    def __call__(self, val_loss, model):\n",
    "\n",
    "        score = -val_loss\n",
    "\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            self.trace_func(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, val_loss, model):\n",
    "        '''Saves model when validation loss decrease.'''\n",
    "        if self.verbose:\n",
    "            self.trace_func(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
    "        torch.save(model.state_dict(), self.path)\n",
    "        self.val_loss_min = val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77d95983",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_time(elapsed):\n",
    "    '''\n",
    "    Takes a time in seconds and returns a string hh:mm:ss\n",
    "    '''\n",
    "    # Round to the nearest second.\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "\n",
    "    # Format as hh:mm:ss\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd93bd5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a97c099c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(D):\n",
    "    if D == 'ARC':\n",
    "        train = pd.read_csv(\"data/train_QC_data.csv\")\n",
    "        val = pd.read_csv(\"data/val_QC_data.csv\")\n",
    "        test = pd.read_csv(\"data/test_QC_data.csv\")\n",
    "        train_features = train[\"Question\"]\n",
    "        test_features = test[\"Question\"]\n",
    "        train_labels = train[\"QCLabel\"]\n",
    "        test_labels = test[\"QCLabel\"]\n",
    "        val_features = val[\"Question\"]\n",
    "        val_labels = val[\"QCLabel\"]\n",
    "    else:\n",
    "        train = pd.read_csv(\"data/train_taxonomy_prediction.csv\")\n",
    "        val = pd.read_csv(\"data/validation_taxonomy_prediction.csv\")\n",
    "        test = pd.read_csv(\"data/test_taxonomy_prediction.csv\")\n",
    "        train_features = train[\"question_answer\"]\n",
    "        test_features = test[\"question_answer\"]\n",
    "        train_labels = train[\"board_syllabus\"]\n",
    "        test_labels = test[\"board_syllabus\"]\n",
    "        val_features = val[\"question_answer\"]\n",
    "        val_labels = val[\"board_syllabus\"]\n",
    "    print('Loading BERT tokenizer...')\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "#     train[\"QCLabel\"].value_counts()\n",
    "#     train_features = train[\"Question\"]\n",
    "#     test_features = test[\"Question\"]\n",
    "#     train_labels = train[\"QCLabel\"]\n",
    "#     test_labels = test[\"QCLabel\"]\n",
    "#     val_features = val[\"Question\"]\n",
    "#     val_labels = val[\"QCLabel\"]\n",
    "    question_answer = train_features.values\n",
    "    categories = train_labels.values\n",
    "    # course_taxonomy\n",
    "    print(\"Cleaning Taxonomy\")\n",
    "    poincare_emb_data = get_cleaned_taxonomy(categories, D)\n",
    "    poincare_val = get_cleaned_taxonomy(val_labels, D)\n",
    "    print(\"Cleaned Taxonomy\")\n",
    "    return train_features, test_features, train_labels, test_labels, val_features, val_labels, question_answer, categories, poincare_emb_data, poincare_val, tokenizer, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d69f2391",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings(poincare_emb_data, poincare_val, question_answer, val_features, tokenizer, categories):\n",
    "    # taxonomy_vectors = []\n",
    "#     taxonomy_vectors = use_embedding.get_tokenized_sents_embeddings_USE(poincare_emb_data)\n",
    "    taxonomy_vectors = sbert.encode(poincare_emb_data)\n",
    "    taxonomy_vectors = np.vstack(taxonomy_vectors)\n",
    "    print(taxonomy_vectors.shape)\n",
    "\n",
    "    # taxonomy_vectors_val = []\n",
    "    # for feature in poincare_val:\n",
    "#     taxonomy_vectors_val = use_embedding.get_tokenized_sents_embeddings_USE(poincare_val)\n",
    "    taxonomy_vectors_val = sbert.encode(poincare_val)\n",
    "    taxonomy_vectors_val = np.vstack(taxonomy_vectors_val)\n",
    "    print(taxonomy_vectors_val.shape)\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "\n",
    "    for sent in question_answer:\n",
    "        # `encode_plus` will:\n",
    "        #   (1) Tokenize the sentence.\n",
    "        #   (2) Prepend the `[CLS]` token to the start.\n",
    "        #   (3) Append the `[SEP]` token to the end.\n",
    "        #   (4) Map tokens to their IDs.\n",
    "        #   (5) Pad or truncate the sentence to `max_length`\n",
    "        #   (6) Create attention masks for [PAD] tokens.\n",
    "        encoded_dict = tokenizer.encode_plus(\n",
    "                            sent,                      # Sentence to encode.\n",
    "                            add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                            max_length = 128,           # Pad & truncate all sentences.\n",
    "                            pad_to_max_length = True,\n",
    "                            truncation=True,\n",
    "                            return_attention_mask = True,   # Construct attn. masks.\n",
    "                            return_tensors = 'pt',     # Return pytorch tensors.\n",
    "                       )\n",
    "\n",
    "        # Add the encoded sentence to the list.    \n",
    "        input_ids.append(encoded_dict['input_ids'])\n",
    "\n",
    "        # And its attention mask (simply differentiates padding from non-padding).\n",
    "        attention_masks.append(encoded_dict['attention_mask'])\n",
    "\n",
    "    # Convert the lists into tensors.\n",
    "    input_ids = torch.cat(input_ids, dim=0)\n",
    "    attention_masks = torch.cat(attention_masks, dim=0)\n",
    "    # Print sentence 0, now as a list of IDs.\n",
    "    print('Original: ', question_answer[0])\n",
    "    print('Token IDs:', input_ids[0])\n",
    "    \n",
    "    input_ids_val = []\n",
    "    attention_masks_val = []\n",
    "\n",
    "    for sent in val_features:\n",
    "        # `encode_plus` will:\n",
    "        #   (1) Tokenize the sentence.\n",
    "        #   (2) Prepend the `[CLS]` token to the start.\n",
    "        #   (3) Append the `[SEP]` token to the end.\n",
    "        #   (4) Map tokens to their IDs.\n",
    "        #   (5) Pad or truncate the sentence to `max_length`\n",
    "        #   (6) Create attention masks for [PAD] tokens.\n",
    "        encoded_dict = tokenizer.encode_plus(\n",
    "                            sent,                      # Sentence to encode.\n",
    "                            add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                            max_length = 128,           # Pad & truncate all sentences.\n",
    "                            pad_to_max_length = True,\n",
    "                            truncation=True,\n",
    "                            return_attention_mask = True,   # Construct attn. masks.\n",
    "                            return_tensors = 'pt',     # Return pytorch tensors.\n",
    "                       )\n",
    "\n",
    "        # Add the encoded sentence to the list.    \n",
    "        input_ids_val.append(encoded_dict['input_ids'])\n",
    "\n",
    "        # And its attention mask (simply differentiates padding from non-padding).\n",
    "        attention_masks_val.append(encoded_dict['attention_mask'])\n",
    "\n",
    "    # Convert the lists into tensors.\n",
    "    input_ids_val = torch.cat(input_ids_val, dim=0)\n",
    "    attention_masks_val = torch.cat(attention_masks_val, dim=0)\n",
    "\n",
    "\n",
    "    # Print sentence 0, now as a list of IDs.\n",
    "    print('Original: ', question_answer[0])\n",
    "    print('Token IDs:', input_ids[0])\n",
    "    \n",
    "    num_classes = len(list(set(categories)))\n",
    "    \n",
    "    train_poincare_tensor = torch.tensor(taxonomy_vectors,dtype=torch.float)\n",
    "    val_poincare_tensor = torch.tensor(taxonomy_vectors_val,dtype=torch.float)\n",
    "\n",
    "    val_dataset = TensorDataset(input_ids_val,attention_masks_val,val_poincare_tensor)\n",
    "    # Combine the training inputs into a TensorDataset.\n",
    "    train_dataset = TensorDataset(input_ids, attention_masks, train_poincare_tensor)\n",
    "    batch_size = 32\n",
    "    train_dataloader = DataLoader(\n",
    "                train_dataset,  # The training samples.\n",
    "                sampler = RandomSampler(train_dataset), # Select batches randomly\n",
    "                batch_size = batch_size # Trains with this batch size.\n",
    "            )\n",
    "\n",
    "    validation_dataloader = DataLoader(\n",
    "                val_dataset, # The validation samples.\n",
    "                sampler = SequentialSampler(val_dataset), \n",
    "                batch_size = batch_size \n",
    "            )\n",
    "    return val_dataset, train_dataset, train_dataloader, validation_dataloader, batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0edaf289",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_initialize(train_dataloader, model_name=None, base_model=ElectraForPreTraining, path=None):\n",
    "#     electra_pretrained/electra_siya_learn_10.pth\n",
    "    model = CustomClassifier(model_name, base_model, path)\n",
    "    model.cuda()\n",
    "    optimizer_1 = torch.optim.AdamW(model.parameters(),\n",
    "      lr = 2e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
    "      eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n",
    "    )\n",
    "    epochs = 30\n",
    "    # Total number of training steps is [number of batches] x [number of epochs]. \n",
    "    total_steps = len(train_dataloader) * epochs\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer_1, \n",
    "                                                num_warmup_steps = 0, # Default value in run_glue.py\n",
    "                                                num_training_steps = total_steps)\n",
    "    criterion = MyHingeLoss(0.1)\n",
    "    return model, optimizer_1, epochs, total_steps, scheduler, criterion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ad576e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_function(model, epochs, train_dataloader, optimizer_1, criterion, scheduler, validation_dataloader, tokenizer):\n",
    "    TEST = None\n",
    "    # This training code is based on the `run_glue.py` script here:\n",
    "    # https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128\n",
    "\n",
    "    # Set the seed value all over the place to make this reproducible.\n",
    "    seed_val = 42\n",
    "\n",
    "    random.seed(seed_val)\n",
    "    np.random.seed(seed_val)\n",
    "    torch.manual_seed(seed_val)\n",
    "    torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "    # We'll store a number of quantities such as training and validation loss, \n",
    "    # validation accuracy, and timings.\n",
    "    training_stats = []\n",
    "\n",
    "    # Measure the total training time for the whole run.\n",
    "    total_t0 = time.time()\n",
    "    early_stopping = EarlyStopping(patience=6, verbose=True)\n",
    "\n",
    "    # For each epoch...\n",
    "    for epoch_i in range(0, epochs):\n",
    "\n",
    "\n",
    "\n",
    "        print(\"\")\n",
    "        print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "        print('Training...')\n",
    "\n",
    "        # Measure how long the training epoch takes.\n",
    "        t0 = time.time()\n",
    "\n",
    "        # Reset the total loss for this epoch.\n",
    "        total_train_loss = 0\n",
    "\n",
    "\n",
    "    #     model.train()\n",
    "\n",
    "        # For each batch of training data...\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "\n",
    "            # Progress update every 40 batches.\n",
    "            if step % 40 == 0 and not step == 0:\n",
    "                # Calculate elapsed time in minutes.\n",
    "                elapsed = format_time(time.time() - t0)\n",
    "\n",
    "                # Report progress.\n",
    "                print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
    "\n",
    "\n",
    "            b_input_ids = batch[0].to(device)\n",
    "            b_input_mask = batch[1].to(device)\n",
    "            b_labels = batch[2].to(device)\n",
    "            TEST = model(b_input_ids, b_input_mask)\n",
    "\n",
    "            model.zero_grad() \n",
    "            optimizer_1.zero_grad()       \n",
    "    #         print(b_input_ids, b_input_mask)\n",
    "    #         print(model)\n",
    "            logits = model(b_input_ids,b_input_mask)\n",
    "\n",
    "            loss = criterion.forward(logits,b_labels)\n",
    "\n",
    "\n",
    "            total_train_loss += loss.item()\n",
    "\n",
    "            # Perform a backward pass to calculate the gradients.\n",
    "            loss.backward()\n",
    "\n",
    "            # Clip the norm of the gradients to 1.0.\n",
    "            # This is to help prevent the \"exploding gradients\" problem.\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "            # Update parameters and take a step using the computed gradient.\n",
    "            # The optimizer dictates the \"update rule\"--how the parameters are\n",
    "            # modified based on their gradients, the learning rate, etc.\n",
    "            optimizer_1.step()\n",
    "\n",
    "            # Update the learning rate.\n",
    "            scheduler.step()\n",
    "\n",
    "        # Calculate the average loss over all of the batches.\n",
    "        avg_train_loss = total_train_loss / len(train_dataloader)            \n",
    "\n",
    "        # Measure how long this epoch took.\n",
    "        training_time = format_time(time.time() - t0)\n",
    "\n",
    "        print(\"\")\n",
    "        print(\"  Average training loss: {0:.3f}\".format(avg_train_loss))\n",
    "        print(\"  Training epcoh took: {:}\".format(training_time))\n",
    "\n",
    "        # ========================================\n",
    "        #               Validation\n",
    "        # ========================================\n",
    "        # After the completion of each training epoch, measure our performance on\n",
    "        # our validation set.\n",
    "\n",
    "        print(\"\")\n",
    "        print(\"Running Validation...\")\n",
    "\n",
    "        t0 = time.time()\n",
    "\n",
    "        # Put the model in evaluation mode--the dropout layers behave differently\n",
    "        # during evaluation.\n",
    "        model.eval()\n",
    "\n",
    "        # Tracking variables \n",
    "        total_eval_f1 = 0\n",
    "        total_eval_loss = 0\n",
    "        nb_eval_steps = 0\n",
    "\n",
    "        # Evaluate data for one epoch\n",
    "        for batch in validation_dataloader:\n",
    "\n",
    "\n",
    "            b_input_ids = batch[0].to(device)\n",
    "            b_input_mask = batch[1].to(device)\n",
    "            b_labels = batch[2].to(device)\n",
    "\n",
    "            # Tell pytorch not to bother with constructing the compute graph during\n",
    "            # the forward pass, since this is only needed for backprop (training).\n",
    "            with torch.no_grad():        \n",
    "\n",
    "              logits = model(b_input_ids,b_input_mask)\n",
    "            loss = criterion(logits,b_labels)\n",
    "\n",
    "\n",
    "            # Accumulate the validation loss.\n",
    "            total_eval_loss += loss.item()\n",
    "            # Move logits and labels to CPU\n",
    "            logits = logits.detach().cpu().numpy().round()\n",
    "            label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "\n",
    "        # Calculate the average loss over all of the batches.\n",
    "        avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
    "        early_stopping(avg_val_loss, model)\n",
    "        if early_stopping.early_stop:\n",
    "          print(\"Early stopping\")\n",
    "          break  \n",
    "        # Measure how long the validation run took.\n",
    "        validation_time = format_time(time.time() - t0)\n",
    "\n",
    "        print(\"  Validation Loss: {0:.3f}\".format(avg_val_loss))\n",
    "        print(\"  Validation took: {:}\".format(validation_time))\n",
    "        output_dir = 'model_euclidean_USE_cos_QC/'\n",
    "        if not os.path.exists(output_dir):\n",
    "            os.makedirs(output_dir)\n",
    "\n",
    "        print(\"Saving model to %s\" % output_dir)\n",
    "        tokenizer.save_pretrained(output_dir)\n",
    "        torch.save(model.state_dict(), os.path.join(output_dir, 'model_weights'))\n",
    "\n",
    "    #     !rm -rf \"/content/drive/My Drive/research_lo_content_taxonomy_classification/model_euclidean_USE_cos_QC\"\n",
    "    #     !mv model_euclidean_USE_cos_QC \"/content/drive/My Drive/research_lo_content_taxonomy_classification/\"\n",
    "        # Record all statistics from this epoch.\n",
    "        training_stats.append(\n",
    "            {\n",
    "                'epoch': epoch_i + 1,\n",
    "                'Training Loss': avg_train_loss,\n",
    "                'Valid. Loss': avg_val_loss,\n",
    "                'Training Time': training_time,\n",
    "                'Validation Time': validation_time\n",
    "            }\n",
    "        )\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"Training complete!\")\n",
    "\n",
    "    print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))\n",
    "    return model, training_stats, optimizer_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47e20dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_stats_show(training_stats, output_dir):\n",
    "\n",
    "    # Display floats with two decimal places.\n",
    "    pd.set_option('precision', 3)\n",
    "\n",
    "    # Create a DataFrame from our training statistics.\n",
    "    df_stats = pd.DataFrame(data=training_stats)\n",
    "\n",
    "    # Use the 'epoch' as the row index.\n",
    "    df_stats = df_stats.set_index('epoch')\n",
    "    df_stats.to_csv(os.path.join(output_dir, 'training_stats.csv'))\n",
    "    # A hack to force the column headers to wrap.\n",
    "    #df = df.style.set_table_styles([dict(selector=\"th\",props=[('max-width', '70px')])])\n",
    "\n",
    "    # Display the table.\n",
    "    print(df_stats)\n",
    "\n",
    "    # Use plot styling from seaborn.\n",
    "    sns.set(style='darkgrid')\n",
    "\n",
    "    # Increase the plot size and font size.\n",
    "    sns.set(font_scale=1.5)\n",
    "    plt.rcParams[\"figure.figsize\"] = (12,6)\n",
    "\n",
    "    # Plot the learning curve.\n",
    "    plt.plot(df_stats['Training Loss'], 'b-o', label=\"Training\")\n",
    "    plt.plot(df_stats['Valid. Loss'], 'g-o', label=\"Validation\")\n",
    "\n",
    "    # Label the plot.\n",
    "    plt.title(\"Training & Validation Loss\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    plt.xticks([1, 2, 3, 4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20])\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a0e8ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getOutputDir(data_name, D):\n",
    "    return f'model_base_bert_sbert_{data_name}_{D}'\n",
    "\n",
    "def saveOutputs(model, tokenizer, data_name, D, training_stats):\n",
    "    output_dir = getOutputDir(data_name, D) + '/'\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "        \n",
    "    training_stats_show(training_stats, output_dir)\n",
    "\n",
    "    print(\"Saving model to %s\" % output_dir)\n",
    "    tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "    torch.save(model.state_dict(), os.path.join(output_dir, 'model_weights'))\n",
    "    \n",
    "#     !zip -r model_euclidean_USE_cos_QC.zip model_euclidean_USE_cos_QC\n",
    "# files.download('model_euclidean_1.zip')\n",
    "\n",
    "def getCheckpointModel(path=None):\n",
    "    trained = CustomClassifier('google/electra-small-discriminator', ElectraForPreTraining)\n",
    "    if not path:\n",
    "        trained.load_state_dict(torch.load('checkpoint.pt'))\n",
    "    else:\n",
    "        trained.load_state_dict(torch.load(path))\n",
    "    trained.cuda()\n",
    "    return trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93c5f70c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### DEFINE CONSTANTS\n",
    "D = 'EM' # Or EM\n",
    "data_name = 'edu_bert_sbert' # Name of the output file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cfa6411",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train_main(D, data_name, path=None):\n",
    "train_features, test_features, train_labels, test_labels, val_features, val_labels, question_answer, categories, poincare_emb_data, poincare_val, tokenizer, test = prepare_data(D)\n",
    "val_dataset, train_dataset, train_dataloader, validation_dataloader, batch_size = get_embeddings(poincare_emb_data, poincare_val, question_answer, val_features, tokenizer, categories)\n",
    "#     return model, trained, test_features, test_labels, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6917e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertForMaskedLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adfe81d7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# model, optimizer_1, epochs, total_steps, scheduler, criterion = model_initialize(train_dataloader, model_name=\"bert-base-uncased\", base_model=BertModel)\n",
    "model, optimizer_1, epochs, total_steps, scheduler, criterion = model_initialize(train_dataloader, model_name=\"./BERT_MAIN_MODEL/\", base_model=BertForMaskedLM)\n",
    "model, training_stats, optimizer_1 = train_function(model, epochs, train_dataloader, optimizer_1, criterion, scheduler, validation_dataloader, tokenizer)\n",
    "saveOutputs(model, tokenizer, data_name, D, training_stats)\n",
    "# trained = getCheckpointModel()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95615744",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model, trained, test_features, test_labels, test = train_main(D, data_name, './electra_pretrained/electra_siya_learn_10.pth')\n",
    "# testCode(model, D, test_features, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0e918ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = getCheckpointModel('model_electron_USE_test_EM/model_weights')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "312bf31b",
   "metadata": {},
   "outputs": [],
   "source": [
    "testCode(model, D, test, test_features, test_labels, 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d119d901",
   "metadata": {},
   "outputs": [],
   "source": [
    "cos = torch.nn.CosineSimilarity(dim=0, eps=1e-6)\n",
    "def dist_without_grad( u, v):\n",
    "  sqdist = torch.sum((u - v) ** 2, dim=-1)\n",
    "  squnorm = torch.sum(u ** 2, dim=-1)\n",
    "  sqvnorm = torch.sum(v ** 2, dim=-1)\n",
    "  x = 1 + 2 * sqdist / ((1 - squnorm) * (1 - sqvnorm)) + 1e-7\n",
    "  z = torch.sqrt(x ** 2 - 1)\n",
    "  return torch.log(x + z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6e2de58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getResultsTopK(model, test_input_ids, test_attention_masks, test_poincare_tensor, targets, LE, test_labels, K=10):\n",
    "    print('Predicting labels for {:,} test sentences...'.format(len(test_input_ids)))\n",
    "    # Put model in evaluation mode\n",
    "    model.eval()\n",
    "    cos = torch.nn.CosineSimilarity(dim=1, eps=1e-6)\n",
    "    input_ids = test_input_ids.to('cuda')\n",
    "    attention_masks = test_attention_masks.to('cuda')\n",
    "    test_poincare_tensor = test_poincare_tensor.to('cuda')\n",
    "    # Tracking variables \n",
    "    predictions , true_labels = [], []\n",
    "    for input_id,attention_mask in zip(input_ids, attention_masks):\n",
    "      with torch.no_grad():\n",
    "        outputs = model(input_id.reshape(1,-1),attention_mask.reshape(1,-1))\n",
    "      distances = cos(outputs,test_poincare_tensor)\n",
    "      distances,indices = torch.topk(distances,K,largest=True)\n",
    "#       print(indices.cpu().numpy())\n",
    "      predictions.append(targets[indices.cpu().numpy()])\n",
    "    print(len(predictions))\n",
    "    final_predictions = []\n",
    "    for prediction in predictions:\n",
    "      final_predictions.append(LE.transform(prediction))\n",
    "    print('    DONE.')\n",
    "    # predictions\n",
    "    y_true = np.array(test_labels)\n",
    "    y_true = tf.identity(y_true)\n",
    "    y_pred = np.array(final_predictions)\n",
    "    y_pred = tf.identity(y_pred)\n",
    "    print(y_pred.shape,y_true.shape)\n",
    "    recall, update_recall = tf.compat.v1.metrics.recall_at_top_k(y_true, y_pred, k=K)\n",
    "    precision, update_precision = tf.compat.v1.metrics.precision_at_top_k(y_true, y_pred, k=K)\n",
    "\n",
    "    tmp_rank = tf.nn.top_k(y_pred, K)\n",
    "    stream_vars = [i for i in tf.local_variables()]\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.local_variables_initializer())\n",
    "    #     print(\"precision\",sess.run(update_precision))\n",
    "    #     print(\"precision\",sess.run(precision))\n",
    "\n",
    "        print(\"update_recall: \",sess.run(update_recall ))\n",
    "        print(\"recall\",sess.run(recall))\n",
    "\n",
    "        print(\"STREAM_VARS: \",(sess.run(stream_vars)))\n",
    "        print(\"TMP_RANK: \",sess.run(y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e629463f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision(actual, predicted, k):\n",
    "    act_set = set(actual)\n",
    "    pred_set = set(predicted[:k])\n",
    "    result = len(act_set & pred_set) / float(k)\n",
    "    return result\n",
    "\n",
    "def recall(actual, predicted, k):\n",
    "    act_set = set(actual)\n",
    "    pred_set = set(predicted[:k])\n",
    "    result = len(act_set & pred_set) / float(len(act_set))\n",
    "    return result\n",
    "\n",
    "def testCode(model, D, test, test_features, test_labels, batch_size):\n",
    "    test_features_vals = test_features.values\n",
    "    labels = test_labels.values\n",
    "    testLabel = list(set(labels))\n",
    "#     targets = pd.read_csv(\"data/targets_ARC.csv\")\n",
    "#     labels = targets['targets'].values\n",
    "#     targets = targets[\"targets\"].values\n",
    "#     # course_taxonomy\n",
    "#     targets = list(set(targets))\n",
    "    poincare_emb_data_test = get_cleaned_taxonomy(testLabel, D)\n",
    "    # taxonomy_vectors = []\n",
    "    taxonomy_vectors = sbert.encode(poincare_emb_data_test)\n",
    "    # taxonomy_vectors = np.vstack(taxonomy_vectors)\n",
    "#     taxonomy_vectors.shape\n",
    "    test_input_ids = []\n",
    "    test_attention_masks = []\n",
    "    for sent in test_features_vals:\n",
    "\n",
    "        encoded_dict = tokenizer.encode_plus(\n",
    "                            sent,                      # Sentence to encode.\n",
    "                            add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                            max_length = 128,           # Pad & truncate all sentences.\n",
    "                            pad_to_max_length = True,\n",
    "                            truncation=True,\n",
    "                            return_attention_mask = True,   # Construct attn. masks.\n",
    "                            return_tensors = 'pt',     # Return pytorch tensors.\n",
    "                       )\n",
    "\n",
    "        # Add the encoded sentence to the list.    \n",
    "        test_input_ids.append(encoded_dict['input_ids'])\n",
    "\n",
    "        # And its attention mask (simply differentiates padding from non-padding).\n",
    "        test_attention_masks.append(encoded_dict['attention_mask'])\n",
    "\n",
    "    # Convert the lists into tensors.\n",
    "    test_input_ids = torch.cat(test_input_ids, dim=0)\n",
    "    test_attention_masks = torch.cat(test_attention_masks, dim=0)\n",
    "\n",
    "    # Set the batch size.  \n",
    "    batch_size=32\n",
    "    test_poincare_tensor = torch.tensor(taxonomy_vectors,dtype=torch.float)\n",
    "#     test_poincare_tensor = torch.tensor(taxonomy_vectors,dtype=torch.float)\n",
    "    testLabel = np.array(testLabel)\n",
    "    LE= LabelEncoder()\n",
    "    testLabel = LE.fit_transform(testLabel)\n",
    "    if D == 'ARC':\n",
    "        testLabels = LE.transform(test[\"QCLabel\"].values)\n",
    "    else:\n",
    "        testLabels = LE.transform(test[\"board_syllabus\"].values)\n",
    "    getResultsTopK(model, test_input_ids, test_attention_masks, test_poincare_tensor, np.array(list(set(labels))), LE, testLabels, K=5)\n",
    "    getResultsTopK(model, test_input_ids, test_attention_masks, test_poincare_tensor, np.array(list(set(labels))), LE, testLabels, K=10)\n",
    "    getResultsTopK(model, test_input_ids, test_attention_masks, test_poincare_tensor, np.array(list(set(labels))), LE, testLabels, K=15)\n",
    "    getResultsTopK(model, test_input_ids, test_attention_masks, test_poincare_tensor, np.array(list(set(labels))), LE, testLabels, K=20)\n",
    "    # Prediction on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d786212",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
